{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvxQ0ITZSqOgVciqPj0uHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johansbustamante-gif/Proyecto-Inteligencia-Artificial/blob/main/04-Modelo%20CatBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7601Lm91byk",
        "outputId": "9bb78f8f-5543-480c-f75c-36be63014e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando Google Drive...\n",
            "Mounted at /content/drive\n",
            "Cargando datos...\n",
            "Train shape: (644232, 21)\n",
            "Test shape: (296786, 20)\n",
            "Categorical cols: 14 Numeric cols: 5\n",
            "Realizando split 70/30 estratificado...\n",
            "Clases: ['alto', 'bajo', 'medio-alto', 'medio-bajo']\n",
            "CatBoost categorical cols: 14\n",
            "Class weights computed (normalized).\n",
            "=== Entrenamiento CatBoost (intentando GPU si USE_GPU=True) ===\n",
            "Intentando CatBoost en GPU...\n",
            "Error entrenando CatBoost en GPU (se intentará CPU). Error: CatBoostError('catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version')\n",
            "Reintentando CatBoost en CPU...\n",
            "0:\tlearn: 1.3742201\ttest: 1.3743282\tbest: 1.3743282 (0)\ttotal: 6.97s\tremaining: 1h 56m 6s\n",
            "100:\tlearn: 1.2028303\ttest: 1.2055966\tbest: 1.2055966 (100)\ttotal: 7m 36s\tremaining: 1h 7m 40s\n",
            "200:\tlearn: 1.1935543\ttest: 1.1983314\tbest: 1.1983314 (200)\ttotal: 15m 29s\tremaining: 1h 1m 35s\n",
            "300:\tlearn: 1.1891704\ttest: 1.1962313\tbest: 1.1962220 (298)\ttotal: 23m 12s\tremaining: 53m 53s\n",
            "400:\tlearn: 1.1864838\ttest: 1.1950024\tbest: 1.1949924 (397)\ttotal: 30m 43s\tremaining: 45m 53s\n",
            "500:\tlearn: 1.1846663\ttest: 1.1943293\tbest: 1.1943293 (500)\ttotal: 38m 35s\tremaining: 38m 26s\n",
            "600:\tlearn: 1.1832580\ttest: 1.1943669\tbest: 1.1942045 (555)\ttotal: 46m 31s\tremaining: 30m 53s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 1.194204504\n",
            "bestIteration = 555\n",
            "\n",
            "Shrink model to first 556 iterations.\n",
            "CatBoost entrenado en CPU correctamente.\n",
            "CatBoost guardado (joblib).\n",
            "Entrenando XGBoost (CPU, DMatrix)...\n",
            "[0]\ttrain-mlogloss:1.37848\teval-mlogloss:1.37957\n",
            "[100]\ttrain-mlogloss:1.21309\teval-mlogloss:1.24119\n",
            "[196]\ttrain-mlogloss:1.19569\teval-mlogloss:1.23936\n",
            "Entrenando LightGBM (LGBMClassifier)...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126194 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1668\n",
            "[LightGBM] [Info] Number of data points in the train set: 450962, number of used features: 48\n",
            "[LightGBM] [Info] Start training from score -1.398067\n",
            "[LightGBM] [Info] Start training from score -1.386405\n",
            "[LightGBM] [Info] Start training from score -1.385882\n",
            "[LightGBM] [Info] Start training from score -1.374957\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's multi_logloss: 1.25081\n",
            "Early stopping, best iteration is:\n",
            "[76]\tvalid_0's multi_logloss: 1.24832\n",
            "Modelos individuales listos.\n",
            "Entrenando MLP (opcional)...\n",
            "Construyendo dataset para meta-learner...\n",
            "Entrenando meta-learner (LogisticRegression)...\n",
            "Accuracy hold-out (stack): 0.4376675117710974\n",
            "Reporte clasificación (stack):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        alto       0.59      0.59      0.59     47752\n",
            "        bajo       0.48      0.54      0.51     48312\n",
            "  medio-alto       0.33      0.31      0.32     48338\n",
            "  medio-bajo       0.34      0.31      0.32     48868\n",
            "\n",
            "    accuracy                           0.44    193270\n",
            "   macro avg       0.43      0.44      0.43    193270\n",
            "weighted avg       0.43      0.44      0.43    193270\n",
            "\n",
            "Submission guardado en: /content/drive/MyDrive/DataProyectoIA/submission_ensemble_fastV2.csv\n",
            "       ID RENDIMIENTO_GLOBAL\n",
            "0  550236               bajo\n",
            "1   98545         medio-bajo\n",
            "2  499179               alto\n",
            "3  782980               bajo\n",
            "4  785185               bajo\n",
            "Artefactos guardados en: /content/models_saberpro\n",
            "Top importancias CatBoost (prettified):\n",
            "                          Feature Id  Importances\n",
            "0                   E_PRGM_ACADEMICO    29.815826\n",
            "1                E_PRGM_DEPARTAMENTO    17.201898\n",
            "2        E_VALORMATRICULAUNIVERSIDAD    16.477810\n",
            "3                  PERIODO_ACADEMICO     5.781725\n",
            "4                   F_EDUCACIONMADRE     5.226001\n",
            "5                   F_EDUCACIONPADRE     4.132543\n",
            "6                  F_ESTRATOVIVIENDA     4.131121\n",
            "7               E_HORASSEMANATRABAJA     3.721609\n",
            "8              E_PAGOMATRICULAPROPIO     1.290947\n",
            "9    E_VALORMATRICULAUNIVERSIDAD_cnt     1.162755\n",
            "10  E_VALORMATRICULAUNIVERSIDAD_freq     0.893434\n",
            "11                 F_TIENECOMPUTADOR     0.812122\n",
            "12              E_PRGM_ACADEMICO_cnt     0.776543\n",
            "13        E_PAGOMATRICULAPROPIO_freq     0.732292\n",
            "14             E_PRGM_ACADEMICO_freq     0.724325\n",
            "15         E_PAGOMATRICULAPROPIO_cnt     0.645753\n",
            "16           E_PRGM_DEPARTAMENTO_cnt     0.608709\n",
            "17                       INDICADOR_2     0.550111\n",
            "18              F_TIENEINTERNET_freq     0.496883\n",
            "19                 F_TIENEINTERNET.1     0.407981\n",
            "Top 20 XGBoost (gain):\n",
            "[('f1', 99.3458023071289), ('f3', 85.08328247070312), ('f14', 44.89804458618164), ('f48', 42.422325134277344), ('f5', 42.252174377441406), ('f13', 38.99492645263672), ('f41', 30.960954666137695), ('f23', 30.058874130249023), ('f7', 28.986825942993164), ('f29', 28.304983139038086), ('f6', 27.22553825378418), ('f2', 26.016387939453125), ('f40', 25.835662841796875), ('f12', 25.084144592285156), ('f11', 25.056095123291016), ('f42', 25.045650482177734), ('f39', 23.92171287536621), ('f4', 21.253896713256836), ('f22', 19.879926681518555), ('f16', 18.387256622314453)]\n",
            "Ejecución completa. Revisa submission_ensemble_fastV2.csv en tu Drive si se generó.\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Notebook completo (celda única) - Split 70/30\n",
        "# CatBoost (GPU opcional, fallback CPU) + XGBoost + LGBM + Stacking\n",
        "# Guarda submission_ensemble_fastV2.csv\n",
        "# =========================\n",
        "\n",
        "# ---------- 0) Instalación mínima (descomenta si necesitas instalar paquetes) ----------\n",
        "import sys, subprocess, os, time\n",
        "def pip_install(packages):\n",
        "    to_install = []\n",
        "    for pkg in packages:\n",
        "        name = pkg.split('==')[0]\n",
        "        try:\n",
        "            __import__(name)\n",
        "        except Exception:\n",
        "            to_install.append(pkg)\n",
        "    if to_install:\n",
        "        print(\"Instalando:\", to_install)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *to_install])\n",
        "    else:\n",
        "        print(\"Dependencias OK\")\n",
        "\n",
        "# Si te faltan paquetes en el runtime descomenta la siguiente línea:\n",
        "# pip_install([\"category_encoders\",\"catboost\",\"xgboost\",\"lightgbm\",\"joblib\",\"scikit-learn\",\"pandas\",\"numpy\",\"seaborn\"])\n",
        "\n",
        "# ---------- 1) Imports ----------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from category_encoders import TargetEncoder\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# ---------- 2) Montar Google Drive ----------\n",
        "from google.colab import drive\n",
        "print(\"Montando Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ---------- 3) Rutas ----------\n",
        "DRIVE_PATH = '/content/drive/MyDrive/DataProyectoIA'\n",
        "TRAIN_PATH = os.path.join(DRIVE_PATH, 'train_limpio.csv')\n",
        "TEST_PATH  = os.path.join(DRIVE_PATH, 'test.csv')\n",
        "OUT_DIR = '/content/models_saberpro'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(TRAIN_PATH):\n",
        "    raise FileNotFoundError(f\"No se encontró {TRAIN_PATH}. Ajusta DRIVE_PATH o sube el archivo.\")\n",
        "\n",
        "# ---------- 4) Cargar datos ----------\n",
        "print(\"Cargando datos...\")\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "print(\"Train shape:\", train.shape)\n",
        "test = None\n",
        "if os.path.exists(TEST_PATH):\n",
        "    test = pd.read_csv(TEST_PATH)\n",
        "    print(\"Test shape:\", test.shape)\n",
        "else:\n",
        "    print(\"No se encontró test.csv. El pipeline correrá sin submission final.\")\n",
        "\n",
        "# ---------- 5) Preparar X, y ----------\n",
        "TARGET = 'RENDIMIENTO_GLOBAL'\n",
        "if TARGET not in train.columns:\n",
        "    raise KeyError(f\"Columna {TARGET} no encontrada en train.\")\n",
        "\n",
        "X_raw = train.drop(columns=[TARGET]).copy()\n",
        "y_raw = train[TARGET].copy()\n",
        "\n",
        "# Guardar ID y remover de X\n",
        "if 'ID' in X_raw.columns:\n",
        "    train_ids = X_raw['ID'].copy()\n",
        "    X_raw = X_raw.drop(columns=['ID'])\n",
        "else:\n",
        "    train_ids = None\n",
        "\n",
        "# Columnas categóricas / numéricas\n",
        "cat_cols = X_raw.select_dtypes(include=['object','category']).columns.tolist()\n",
        "num_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(\"Categorical cols:\", len(cat_cols), \"Numeric cols:\", len(num_cols))\n",
        "\n",
        "# ---------- 6) Split 70/30 estratificado ----------\n",
        "print(\"Realizando split 70/30 estratificado...\")\n",
        "le_tmp = LabelEncoder()\n",
        "y_tmp = le_tmp.fit_transform(y_raw)\n",
        "X_train_raw, X_val_raw, y_train_tmp, y_val_tmp = train_test_split(\n",
        "    X_raw, y_tmp, test_size=0.30, random_state=42, stratify=y_tmp\n",
        ")\n",
        "\n",
        "# Etiquetado final consistente con strings\n",
        "le = LabelEncoder()\n",
        "y_train_labels = le.fit_transform(le_tmp.inverse_transform(y_train_tmp))\n",
        "y_val_labels   = le.transform(le_tmp.inverse_transform(y_val_tmp))\n",
        "print(\"Clases:\", list(le.classes_))\n",
        "\n",
        "# Variables de entrenamiento/validación\n",
        "y_train_enc = np.array(y_train_labels)\n",
        "y_val_enc   = np.array(y_val_labels)\n",
        "\n",
        "# ---------- 7) Ingeniería de features ----------\n",
        "def feature_engineering(df):\n",
        "    df = df.copy()\n",
        "    # count / freq para categóricas\n",
        "    for c in cat_cols:\n",
        "        if c in df.columns:\n",
        "            vc = df[c].value_counts(dropna=False)\n",
        "            df[c + \"_cnt\"] = df[c].map(vc).fillna(0).astype(int)\n",
        "            df[c + \"_freq\"] = df[c + \"_cnt\"] / len(df)\n",
        "    # ejemplos de interacciones / transformaciones (ajusta según columnas reales)\n",
        "    if 'INGRESO_HOGAR' in df.columns and 'MUNICIPIO' in df.columns:\n",
        "        df['ingreso_rank_mun'] = df.groupby('MUNICIPIO')['INGRESO_HOGAR'].rank(pct=True).fillna(0)\n",
        "    if 'EDAD' in df.columns and 'ESTRATO' in df.columns:\n",
        "        df['edad_x_estrato'] = df['EDAD'] * df['ESTRATO']\n",
        "    if 'EDAD' in df.columns:\n",
        "        df['edad_sq'] = df['EDAD']**2\n",
        "    # rare levels\n",
        "    for c in cat_cols:\n",
        "        if c in df.columns:\n",
        "            freq = df[c].map(df[c].value_counts(normalize=True))\n",
        "            df[c + \"_rare\"] = (freq < 0.01).astype(int)\n",
        "    # rellenar NA\n",
        "    df = df.fillna(-999)\n",
        "    return df\n",
        "\n",
        "X_train_fe = feature_engineering(X_train_raw)\n",
        "X_val_fe   = feature_engineering(X_val_raw)\n",
        "if test is not None:\n",
        "    if 'ID' in test.columns:\n",
        "        test_ids = test['ID']\n",
        "        X_test_raw = test.drop(columns=['ID'])\n",
        "    else:\n",
        "        test_ids = None\n",
        "        X_test_raw = test.copy()\n",
        "    X_test_fe = feature_engineering(X_test_raw)\n",
        "else:\n",
        "    X_test_fe = None\n",
        "\n",
        "# ---------- 8) Preprocesado: TargetEncoder + Scaler ----------\n",
        "cat_cols_cb = [c for c in cat_cols if c in X_train_fe.columns]\n",
        "print(\"CatBoost categorical cols:\", len(cat_cols_cb))\n",
        "\n",
        "te_cols = [c for c in cat_cols if c in X_train_fe.columns]\n",
        "te = TargetEncoder(cols=te_cols, smoothing=0.3)\n",
        "X_train_te = te.fit_transform(X_train_fe, y_train_enc)\n",
        "X_val_te   = te.transform(X_val_fe)\n",
        "X_test_te  = te.transform(X_test_fe) if X_test_fe is not None else None\n",
        "\n",
        "# Escalar numéricos (útil para MLP/meta modelos)\n",
        "scaler = StandardScaler()\n",
        "numeric_for_scaler = [c for c in X_train_te.select_dtypes(include=[np.number]).columns.tolist()]\n",
        "if len(numeric_for_scaler) > 0:\n",
        "    X_train_te[numeric_for_scaler] = scaler.fit_transform(X_train_te[numeric_for_scaler])\n",
        "    X_val_te[numeric_for_scaler]   = scaler.transform(X_val_te[numeric_for_scaler])\n",
        "    if X_test_te is not None:\n",
        "        X_test_te[numeric_for_scaler] = scaler.transform(X_test_te[numeric_for_scaler])\n",
        "\n",
        "# Arrays para XGB/LGB/MLP\n",
        "X_train_xgb = X_train_te.values\n",
        "X_val_xgb   = X_val_te.values\n",
        "X_test_xgb  = X_test_te.values if X_test_te is not None else None\n",
        "\n",
        "# ---------- 9) Pesos de clase (opcional) ----------\n",
        "counter = Counter(y_train_enc)\n",
        "class_weights = {cls: 1.0/np.sqrt(count) for cls,count in counter.items()}\n",
        "weights_list = np.array([class_weights[int(lbl)] for lbl in y_train_enc])\n",
        "weights_list = weights_list / np.mean(weights_list)\n",
        "class_weight_list = [class_weights.get(i,1.0) for i in range(len(le.classes_))]\n",
        "print(\"Class weights computed (normalized).\")\n",
        "\n",
        "# ---------- 10) Entrenamiento CatBoost robusto (Pools con weight; intenta GPU -> fallback CPU) ----------\n",
        "USE_GPU = True   # Pon False si quieres forzar CPU\n",
        "catboost_params = {\n",
        "    \"iterations\": 1000,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"depth\": 6,\n",
        "    \"loss_function\": \"MultiClass\",\n",
        "    \"eval_metric\": \"MultiClass\",\n",
        "    \"random_seed\": 42,\n",
        "    \"early_stopping_rounds\": 50,\n",
        "    \"verbose\": 100\n",
        "}\n",
        "\n",
        "print(\"=== Entrenamiento CatBoost (intentando GPU si USE_GPU=True) ===\")\n",
        "\n",
        "# Validar weights_list\n",
        "if weights_list is None or len(weights_list) != len(y_train_enc):\n",
        "    print(\"Advertencia: 'weights_list' inválido o longitud diferente. No se usarán weights.\")\n",
        "    weights_list_local = None\n",
        "else:\n",
        "    weights_list_local = weights_list\n",
        "\n",
        "# Función para crear Pools (con weights opcionalmente)\n",
        "def make_pools(use_weights):\n",
        "    if use_weights and (weights_list_local is not None):\n",
        "        train_pool = Pool(data=X_train_fe, label=y_train_enc, cat_features=cat_cols_cb, weight=weights_list_local)\n",
        "    else:\n",
        "        train_pool = Pool(data=X_train_fe, label=y_train_enc, cat_features=cat_cols_cb)\n",
        "    val_pool = Pool(data=X_val_fe, label=y_val_enc, cat_features=cat_cols_cb)\n",
        "    test_pool = Pool(data=X_test_fe, label=None, cat_features=cat_cols_cb) if X_test_fe is not None else None\n",
        "    return train_pool, val_pool, test_pool\n",
        "\n",
        "# Función de entrenamiento usando Pools (no pasar sample_weight en fit cuando usamos Pool)\n",
        "def train_catboost_with_pool(params, use_gpu, use_weights_for_pool=True):\n",
        "    params_local = params.copy()\n",
        "    if use_gpu:\n",
        "        params_local.update({\"task_type\":\"GPU\", \"devices\":\"0\"})\n",
        "    else:\n",
        "        params_local.pop(\"task_type\", None)\n",
        "        params_local.pop(\"devices\", None)\n",
        "    model_local = CatBoostClassifier(**params_local)\n",
        "    tr_pool, va_pool, te_pool = make_pools(use_weights_for_pool)\n",
        "    model_local.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    return model_local\n",
        "\n",
        "model_cb = None\n",
        "if USE_GPU:\n",
        "    try:\n",
        "        print(\"Intentando CatBoost en GPU...\")\n",
        "        model_cb = train_catboost_with_pool(catboost_params, use_gpu=True, use_weights_for_pool=True)\n",
        "        print(\"CatBoost entrenado en GPU correctamente.\")\n",
        "    except Exception as e_gpu:\n",
        "        print(\"Error entrenando CatBoost en GPU (se intentará CPU). Error:\", repr(e_gpu))\n",
        "        try:\n",
        "            print(\"Reintentando CatBoost en CPU...\")\n",
        "            model_cb = train_catboost_with_pool(catboost_params, use_gpu=False, use_weights_for_pool=True)\n",
        "            print(\"CatBoost entrenado en CPU correctamente.\")\n",
        "        except Exception as e_cpu:\n",
        "            print(\"Error CatBoost CPU:\", repr(e_cpu))\n",
        "            raise e_cpu\n",
        "else:\n",
        "    try:\n",
        "        print(\"Entrenando CatBoost en CPU...\")\n",
        "        model_cb = train_catboost_with_pool(catboost_params, use_gpu=False, use_weights_for_pool=True)\n",
        "        print(\"CatBoost entrenado en CPU correctamente.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error CatBoost CPU:\", repr(e))\n",
        "        raise e\n",
        "\n",
        "# Probabilidades CatBoost\n",
        "probs_cb_val  = model_cb.predict_proba(X_val_fe)\n",
        "probs_cb_test = model_cb.predict_proba(X_test_fe) if X_test_fe is not None else None\n",
        "\n",
        "# Guardar CatBoost (intenta joblib, si falla usa save_model)\n",
        "try:\n",
        "    joblib.dump(model_cb, os.path.join(OUT_DIR, \"catboost_model_safe.joblib\"))\n",
        "    print(\"CatBoost guardado (joblib).\")\n",
        "except Exception:\n",
        "    try:\n",
        "        model_cb.save_model(os.path.join(OUT_DIR, \"catboost_model_safe.cbm\"))\n",
        "        print(\"CatBoost guardado (.cbm).\")\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo guardar CatBoost:\", e)\n",
        "\n",
        "# ---------- 11) Entrenamiento XGBoost (CPU, DMatrix) ----------\n",
        "print(\"Entrenando XGBoost (CPU, DMatrix)...\")\n",
        "dtrain = xgb.DMatrix(X_train_xgb, label=y_train_enc)\n",
        "dval   = xgb.DMatrix(X_val_xgb, label=y_val_enc)\n",
        "xgb_params = {\n",
        "    \"objective\":\"multi:softprob\",\n",
        "    \"num_class\": len(le.classes_),\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 6,\n",
        "    \"subsample\": 0.85,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "    \"verbosity\": 0\n",
        "}\n",
        "bst = xgb.train(xgb_params, dtrain, num_boost_round=600, evals=[(dtrain,\"train\"), (dval,\"eval\")], early_stopping_rounds=50, verbose_eval=100)\n",
        "probs_xgb_val  = bst.predict(dval)\n",
        "probs_xgb_test = bst.predict(xgb.DMatrix(X_test_xgb)) if X_test_xgb is not None else None\n",
        "\n",
        "# Guardar XGBoost\n",
        "try:\n",
        "    bst.save_model(os.path.join(OUT_DIR, \"xgb_model.json\"))\n",
        "except Exception as e:\n",
        "    print(\"No se pudo guardar XGBoost:\", e)\n",
        "\n",
        "# ---------- 12) Entrenamiento LightGBM (LGBMClassifier) - usa callbacks en vez de early_stopping_rounds ----------\n",
        "print(\"Entrenando LightGBM (LGBMClassifier)...\")\n",
        "lgbm = LGBMClassifier(\n",
        "    objective=\"multiclass\",\n",
        "    num_class=len(le.classes_),\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=1500,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Reemplazamos early_stopping_rounds por callbacks (compatible con varias versiones de lightgbm)\n",
        "lgbm.fit(\n",
        "    X_train_xgb,\n",
        "    y_train_enc,\n",
        "    eval_set=[(X_val_xgb, y_val_enc)],\n",
        "    eval_metric=\"multi_logloss\",\n",
        "    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(period=100)]\n",
        ")\n",
        "\n",
        "probs_lgb_val  = lgbm.predict_proba(X_val_xgb)\n",
        "probs_lgb_test = lgbm.predict_proba(X_test_xgb) if X_test_xgb is not None else None\n",
        "\n",
        "# Guardar LGBM\n",
        "try:\n",
        "    joblib.dump(lgbm, os.path.join(OUT_DIR, \"lgbm_model_safeV2.joblib\"))\n",
        "except Exception as e:\n",
        "    print(\"No se pudo guardar LGBM:\", e)\n",
        "\n",
        "print(\"Modelos individuales listos.\")\n",
        "\n",
        "# ---------- 13) MLP ligero (opcional) ----------\n",
        "use_mlp = False\n",
        "probs_mlp_val = None\n",
        "probs_mlp_test = None\n",
        "try:\n",
        "    print(\"Entrenando MLP (opcional)...\")\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(256,128), max_iter=200, random_state=42, verbose=False)\n",
        "    mlp.fit(X_train_xgb, y_train_enc)\n",
        "    probs_mlp_val = mlp.predict_proba(X_val_xgb)\n",
        "    probs_mlp_test = mlp.predict_proba(X_test_xgb) if X_test_xgb is not None else None\n",
        "    use_mlp = True\n",
        "    joblib.dump(mlp, os.path.join(OUT_DIR, \"mlp_model.joblib\"))\n",
        "except Exception as e:\n",
        "    print(\"MLP omitido por memoria/tiempo:\", e)\n",
        "    use_mlp = False\n",
        "\n",
        "# ---------- 14) Stacking - preparar features para meta (validation) ----------\n",
        "print(\"Construyendo dataset para meta-learner...\")\n",
        "stack_features_val = np.hstack([probs_cb_val, probs_xgb_val, probs_lgb_val])\n",
        "if use_mlp and (probs_mlp_val is not None):\n",
        "    stack_features_val = np.hstack([stack_features_val, probs_mlp_val])\n",
        "y_meta = y_val_enc\n",
        "\n",
        "print(\"Entrenando meta-learner (LogisticRegression)...\")\n",
        "meta = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
        "meta.fit(stack_features_val, y_meta)\n",
        "joblib.dump(meta, os.path.join(OUT_DIR, \"stacker_meta.joblib\"))\n",
        "\n",
        "# ---------- 15) Evaluación hold-out ----------\n",
        "stack_val_preds = meta.predict(stack_features_val)\n",
        "acc_stack_val = accuracy_score(y_meta, stack_val_preds)\n",
        "print(\"Accuracy hold-out (stack):\", acc_stack_val)\n",
        "print(\"Reporte clasificación (stack):\\n\", classification_report(y_meta, stack_val_preds, target_names=le.classes_))\n",
        "\n",
        "# ---------- 16) Generar submission_ensemble_fastV2.csv ----------\n",
        "if X_test_xgb is not None and probs_cb_test is not None and probs_xgb_test is not None and probs_lgb_test is not None:\n",
        "    stack_features_test = np.hstack([probs_cb_test, probs_xgb_test, probs_lgb_test])\n",
        "    if use_mlp and (probs_mlp_test is not None):\n",
        "        stack_features_test = np.hstack([stack_features_test, probs_mlp_test])\n",
        "    preds_test_idx = meta.predict(stack_features_test)\n",
        "    preds_test_labels = le.inverse_transform(preds_test_idx)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        \"ID\": test_ids.values if (test_ids is not None) else test['ID'],\n",
        "        \"RENDIMIENTO_GLOBAL\": preds_test_labels\n",
        "    })\n",
        "\n",
        "    sub_path = os.path.join(DRIVE_PATH, \"submission_ensemble_fastV2.csv\")\n",
        "    submission.to_csv(sub_path, index=False)\n",
        "    print(\"Submission guardado en:\", sub_path)\n",
        "    print(submission.head())\n",
        "else:\n",
        "    print(\"No hay test completo o faltan probabilidades; no se generó submission.\")\n",
        "\n",
        "# ---------- 17) Guardar artefactos y preprocesadores ----------\n",
        "try:\n",
        "    joblib.dump(model_cb, os.path.join(OUT_DIR, \"catboost_model_safe.joblib\"))\n",
        "except Exception:\n",
        "    try:\n",
        "        model_cb.save_model(os.path.join(OUT_DIR,\"catboost_model_safe.cbm\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "try:\n",
        "    bst.save_model(os.path.join(OUT_DIR,\"xgb_model.json\"))\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    joblib.dump(lgbm, os.path.join(OUT_DIR,\"lgbm_model_safeV2.joblib\"))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "joblib.dump(te, os.path.join(OUT_DIR, \"target_encoder.joblib\"))\n",
        "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
        "joblib.dump(le, os.path.join(OUT_DIR, \"label_encoder.joblib\"))\n",
        "print(\"Artefactos guardados en:\", OUT_DIR)\n",
        "\n",
        "# ---------- 18) Diagnóstico rápido ----------\n",
        "try:\n",
        "    print(\"Top importancias CatBoost (prettified):\")\n",
        "    print(model_cb.get_feature_importance(prettified=True)[:20])\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    importances_xgb = bst.get_score(importance_type='gain')\n",
        "    print(\"Top 20 XGBoost (gain):\")\n",
        "    print(sorted(importances_xgb.items(), key=lambda x: x[1], reverse=True)[:20])\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"Ejecución completa. Revisa submission_ensemble_fastV2.csv en tu Drive si se generó.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}