{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd3hqAu3ob7BHB/OUptW88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johansbustamante-gif/Proyecto-Inteligencia-Artificial/blob/main/Modelo%20Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq8bynMy-E1_",
        "outputId": "e0e45cfc-c152-42a8-8c88-c8ee37ba8a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencias OK\n",
            "Montando Google Drive...\n",
            "Mounted at /content/drive\n",
            "Cargando: /content/drive/MyDrive/DataProyectoIA/train_limpio.csv\n",
            "Train shape: (644232, 21)\n",
            "Cargando: /content/drive/MyDrive/DataProyectoIA/test.csv\n",
            "Test shape: (296786, 20)\n",
            "Clases: ['alto', 'bajo', 'medio-alto', 'medio-bajo']\n",
            "Num cols: 5 Cat cols: 14\n",
            "Split realizado (70/30): (450962, 19) (193270, 19)\n",
            "[0]\tvalidation_0-mlogloss:1.37815\n",
            "[50]\tvalidation_0-mlogloss:1.24427\n",
            "[100]\tvalidation_0-mlogloss:1.22591\n",
            "[150]\tvalidation_0-mlogloss:1.21933\n",
            "[200]\tvalidation_0-mlogloss:1.21559\n",
            "[250]\tvalidation_0-mlogloss:1.21284\n",
            "[299]\tvalidation_0-mlogloss:1.21101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-23 02:21:59,116] A new study created in memory with name: xgb_opt_fast\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy validación (baseline): 0.42692606198582295\n",
            "Preparando Optuna rápido (submuestra). Esto tarda menos que explorar todo el dataset.\n",
            "Ejecutando Optuna (rápido). Timeout: 3600 s. Trials máximos: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-23 02:22:42,478] Trial 0 finished with value: 0.4246666666666667 and parameters: {'eta': 0.045464867828470444, 'max_depth': 6, 'subsample': 0.8469283365722686, 'colsample_bytree': 0.9777576695519508, 'lambda': 1.154878748794114, 'alpha': 0.06781806088084787, 'min_child_weight': 7}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:23:27,192] Trial 1 finished with value: 0.4076666666666667 and parameters: {'eta': 0.001836030198941853, 'max_depth': 4, 'subsample': 0.7115406951451138, 'colsample_bytree': 0.6784829792068909, 'lambda': 1.6332501299920132, 'alpha': 0.14056964687152326, 'min_child_weight': 5}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:24:15,195] Trial 2 finished with value: 0.42343333333333333 and parameters: {'eta': 0.04994466840077553, 'max_depth': 5, 'subsample': 0.9110736036665124, 'colsample_bytree': 0.8838252134873121, 'lambda': 0.24548978780772765, 'alpha': 4.733561799652069, 'min_child_weight': 8}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:24:23,646] Trial 3 finished with value: 0.41996666666666665 and parameters: {'eta': 0.17769333000730922, 'max_depth': 8, 'subsample': 0.8616567986338884, 'colsample_bytree': 0.5451723183098358, 'lambda': 0.8670489853861296, 'alpha': 0.09342891792462325, 'min_child_weight': 6}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:25:22,049] Trial 4 finished with value: 0.4219 and parameters: {'eta': 0.005242113583453652, 'max_depth': 6, 'subsample': 0.7870785706057022, 'colsample_bytree': 0.5170049852368036, 'lambda': 0.011846745385057063, 'alpha': 0.1394086477789518, 'min_child_weight': 5}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:25:47,559] Trial 5 finished with value: 0.4246666666666667 and parameters: {'eta': 0.0752644666272795, 'max_depth': 7, 'subsample': 0.700930313837974, 'colsample_bytree': 0.5152799154285683, 'lambda': 0.024069450055044416, 'alpha': 0.03481577971347424, 'min_child_weight': 8}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:26:14,095] Trial 6 finished with value: 0.4236666666666667 and parameters: {'eta': 0.132310461228251, 'max_depth': 5, 'subsample': 0.9404621195563789, 'colsample_bytree': 0.5837540979460749, 'lambda': 0.009776837095600493, 'alpha': 0.0012989872727541248, 'min_child_weight': 5}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:27:43,001] Trial 7 finished with value: 0.4213 and parameters: {'eta': 0.004158167862807945, 'max_depth': 9, 'subsample': 0.8565282672902204, 'colsample_bytree': 0.8768622368761819, 'lambda': 0.08294177930245243, 'alpha': 0.015031682480828775, 'min_child_weight': 6}. Best is trial 0 with value: 0.4246666666666667.\n",
            "[I 2025-11-23 02:28:28,776] Trial 8 finished with value: 0.4265 and parameters: {'eta': 0.044646738207834706, 'max_depth': 5, 'subsample': 0.8313252452791247, 'colsample_bytree': 0.5186824292420609, 'lambda': 3.693183530847594, 'alpha': 2.871816821229357, 'min_child_weight': 1}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:29:13,598] Trial 9 finished with value: 0.4056666666666667 and parameters: {'eta': 0.0013587415937949007, 'max_depth': 4, 'subsample': 0.7323913154816637, 'colsample_bytree': 0.6628830186493052, 'lambda': 0.026100573799398778, 'alpha': 0.577174390744609, 'min_child_weight': 9}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:30:43,649] Trial 10 finished with value: 0.42236666666666667 and parameters: {'eta': 0.021720987978246776, 'max_depth': 10, 'subsample': 0.6295348387019255, 'colsample_bytree': 0.7532954336442458, 'lambda': 9.034690569382969, 'alpha': 8.456106592018754, 'min_child_weight': 1}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:31:35,687] Trial 11 finished with value: 0.4245333333333333 and parameters: {'eta': 0.02620565475557313, 'max_depth': 6, 'subsample': 0.9876970826450783, 'colsample_bytree': 0.9374524946438096, 'lambda': 9.436828351095759, 'alpha': 1.1043709842829668, 'min_child_weight': 1}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:32:41,190] Trial 12 finished with value: 0.4242 and parameters: {'eta': 0.009794125641977847, 'max_depth': 7, 'subsample': 0.8150918247877369, 'colsample_bytree': 0.7799723571874708, 'lambda': 1.4603546354359864, 'alpha': 0.005622040475153366, 'min_child_weight': 3}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:33:32,343] Trial 13 finished with value: 0.42546666666666666 and parameters: {'eta': 0.03773426958859271, 'max_depth': 6, 'subsample': 0.816002971159903, 'colsample_bytree': 0.9973321077225573, 'lambda': 0.2993846863029101, 'alpha': 0.5489955925736716, 'min_child_weight': 3}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:34:24,709] Trial 14 finished with value: 0.4226666666666667 and parameters: {'eta': 0.013031974072889103, 'max_depth': 5, 'subsample': 0.7711730570943055, 'colsample_bytree': 0.8352551777266024, 'lambda': 0.0010794530016142632, 'alpha': 1.6389808012761693, 'min_child_weight': 3}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:35:21,847] Trial 15 finished with value: 0.4240333333333333 and parameters: {'eta': 0.07559463160273636, 'max_depth': 8, 'subsample': 0.9072254968122828, 'colsample_bytree': 0.6512435734052568, 'lambda': 0.16860062876664192, 'alpha': 0.663512942490667, 'min_child_weight': 3}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:36:10,784] Trial 16 finished with value: 0.4251333333333333 and parameters: {'eta': 0.028849035088266667, 'max_depth': 5, 'subsample': 0.6530286422707803, 'colsample_bytree': 0.9961101679139804, 'lambda': 0.488148001286664, 'alpha': 2.834359254829359, 'min_child_weight': 2}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:37:10,079] Trial 17 finished with value: 0.42386666666666667 and parameters: {'eta': 0.007839838387886183, 'max_depth': 6, 'subsample': 0.7618698509488631, 'colsample_bytree': 0.5929014514382631, 'lambda': 2.981891778772109, 'alpha': 0.3522126688514529, 'min_child_weight': 2}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:37:48,794] Trial 18 finished with value: 0.4257666666666667 and parameters: {'eta': 0.10159593097287731, 'max_depth': 4, 'subsample': 0.8202769515641416, 'colsample_bytree': 0.7054100235269888, 'lambda': 4.307925147988123, 'alpha': 0.2667444530499067, 'min_child_weight': 4}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:38:31,569] Trial 19 finished with value: 0.42523333333333335 and parameters: {'eta': 0.09197019619430453, 'max_depth': 4, 'subsample': 0.8891935498693063, 'colsample_bytree': 0.7132202014023623, 'lambda': 3.581801904723203, 'alpha': 2.8561040436008827, 'min_child_weight': 4}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:38:52,401] Trial 20 finished with value: 0.4265 and parameters: {'eta': 0.177315872663239, 'max_depth': 4, 'subsample': 0.9619240175180186, 'colsample_bytree': 0.6057639013958347, 'lambda': 3.9657356318639794, 'alpha': 0.2546272841715729, 'min_child_weight': 10}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:39:12,982] Trial 21 finished with value: 0.4251 and parameters: {'eta': 0.1967089590365753, 'max_depth': 4, 'subsample': 0.9891024735289113, 'colsample_bytree': 0.590760754414273, 'lambda': 4.109095654170458, 'alpha': 0.2599650845071376, 'min_child_weight': 7}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:39:47,182] Trial 22 finished with value: 0.4257 and parameters: {'eta': 0.11944436354157517, 'max_depth': 4, 'subsample': 0.9450812131695787, 'colsample_bytree': 0.6313040065506447, 'lambda': 5.210386116419733, 'alpha': 0.032179731100855216, 'min_child_weight': 10}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:40:32,610] Trial 23 finished with value: 0.425 and parameters: {'eta': 0.05754083635494273, 'max_depth': 5, 'subsample': 0.9500575741558228, 'colsample_bytree': 0.7071010914129051, 'lambda': 0.6256280451282834, 'alpha': 1.3155242336258173, 'min_child_weight': 4}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:41:00,635] Trial 24 finished with value: 0.42586666666666667 and parameters: {'eta': 0.121061181242184, 'max_depth': 4, 'subsample': 0.8235248504793456, 'colsample_bytree': 0.5533260319710587, 'lambda': 2.2897105738544985, 'alpha': 0.23657601708809728, 'min_child_weight': 10}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:41:25,933] Trial 25 finished with value: 0.42393333333333333 and parameters: {'eta': 0.14457477440377017, 'max_depth': 5, 'subsample': 0.884444518904225, 'colsample_bytree': 0.5632936252736475, 'lambda': 2.472556006705933, 'alpha': 7.201778213113437, 'min_child_weight': 10}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:42:09,847] Trial 26 finished with value: 0.4226666666666667 and parameters: {'eta': 0.01752522577079099, 'max_depth': 4, 'subsample': 0.7397925657112577, 'colsample_bytree': 0.6233135477331149, 'lambda': 8.802300899350218, 'alpha': 0.006214756438011213, 'min_child_weight': 9}. Best is trial 8 with value: 0.4265.\n",
            "[I 2025-11-23 02:42:54,227] Trial 27 finished with value: 0.4271 and parameters: {'eta': 0.0643977284744896, 'max_depth': 5, 'subsample': 0.8444842400189495, 'colsample_bytree': 0.5018716873525099, 'lambda': 0.08022373597147348, 'alpha': 0.04865373603675046, 'min_child_weight': 9}. Best is trial 27 with value: 0.4271.\n",
            "[I 2025-11-23 02:43:39,295] Trial 28 finished with value: 0.42673333333333335 and parameters: {'eta': 0.03498350598558001, 'max_depth': 5, 'subsample': 0.9617106260832321, 'colsample_bytree': 0.5132676609005857, 'lambda': 0.07210568237379496, 'alpha': 0.04083355246643675, 'min_child_weight': 9}. Best is trial 27 with value: 0.4271.\n",
            "[I 2025-11-23 02:44:31,951] Trial 29 finished with value: 0.4265333333333333 and parameters: {'eta': 0.03855684786172045, 'max_depth': 6, 'subsample': 0.8428503744148967, 'colsample_bytree': 0.5051804735494889, 'lambda': 0.07879964076885489, 'alpha': 0.0461729060400783, 'min_child_weight': 8}. Best is trial 27 with value: 0.4271.\n",
            "[I 2025-11-23 02:45:19,932] Trial 30 finished with value: 0.42743333333333333 and parameters: {'eta': 0.03685349372490847, 'max_depth': 7, 'subsample': 0.8736633266374918, 'colsample_bytree': 0.5478973934964821, 'lambda': 0.05254994286772205, 'alpha': 0.04490229168860766, 'min_child_weight': 8}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:46:09,885] Trial 31 finished with value: 0.4265 and parameters: {'eta': 0.0357709533448421, 'max_depth': 7, 'subsample': 0.8722798324771699, 'colsample_bytree': 0.5016279556017997, 'lambda': 0.07632325989629, 'alpha': 0.053061221273723265, 'min_child_weight': 8}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:46:36,103] Trial 32 finished with value: 0.42583333333333334 and parameters: {'eta': 0.060622444919995626, 'max_depth': 8, 'subsample': 0.9066538454104173, 'colsample_bytree': 0.5399071777553246, 'lambda': 0.04356792033067487, 'alpha': 0.020055393661276674, 'min_child_weight': 9}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:47:31,333] Trial 33 finished with value: 0.4267 and parameters: {'eta': 0.018235782558719296, 'max_depth': 6, 'subsample': 0.8393454594278128, 'colsample_bytree': 0.5009402899219925, 'lambda': 0.15413099242302014, 'alpha': 0.010267983661807287, 'min_child_weight': 7}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:48:31,357] Trial 34 finished with value: 0.4264 and parameters: {'eta': 0.016158362579520875, 'max_depth': 7, 'subsample': 0.7963917004116522, 'colsample_bytree': 0.5500075365864827, 'lambda': 0.17425023848669402, 'alpha': 0.010835842319470126, 'min_child_weight': 7}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:49:25,637] Trial 35 finished with value: 0.4262666666666667 and parameters: {'eta': 0.019780839796633243, 'max_depth': 6, 'subsample': 0.9242101405132516, 'colsample_bytree': 0.5367040477178775, 'lambda': 0.00523389933396045, 'alpha': 0.0856797262421152, 'min_child_weight': 7}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:50:35,680] Trial 36 finished with value: 0.4261666666666667 and parameters: {'eta': 0.011219215634243216, 'max_depth': 8, 'subsample': 0.8612939218983845, 'colsample_bytree': 0.5707083765193158, 'lambda': 0.04460267808421772, 'alpha': 0.0034900245691229276, 'min_child_weight': 9}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:51:32,879] Trial 37 finished with value: 0.4272666666666667 and parameters: {'eta': 0.02767472304383364, 'max_depth': 7, 'subsample': 0.8892548320102319, 'colsample_bytree': 0.529792214701655, 'lambda': 0.12890327400355223, 'alpha': 0.024086863658959574, 'min_child_weight': 6}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:52:23,221] Trial 38 finished with value: 0.4261666666666667 and parameters: {'eta': 0.028220182406613604, 'max_depth': 9, 'subsample': 0.9195514861121261, 'colsample_bytree': 0.619745017365648, 'lambda': 0.014541079389485895, 'alpha': 0.12809284315395, 'min_child_weight': 6}. Best is trial 30 with value: 0.42743333333333333.\n",
            "[I 2025-11-23 02:52:59,027] Trial 39 finished with value: 0.4266333333333333 and parameters: {'eta': 0.05200428391208205, 'max_depth': 7, 'subsample': 0.9667051600369584, 'colsample_bytree': 0.5278418902276664, 'lambda': 0.04600548177917216, 'alpha': 0.023859354537208464, 'min_child_weight': 8}. Best is trial 30 with value: 0.42743333333333333.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna rápido finalizado. Tiempo (s): 1859\n",
            "Trials completados: 40\n",
            "Mejor accuracy (submuestra): 0.42743333333333333\n",
            "Mejores parámetros (submuestra): {'eta': 0.03685349372490847, 'max_depth': 7, 'subsample': 0.8736633266374918, 'colsample_bytree': 0.5478973934964821, 'lambda': 0.05254994286772205, 'alpha': 0.04490229168860766, 'min_child_weight': 8}\n",
            "Reentrenando XGBoost final sobre train+val con mejores parámetros...\n",
            "num_boost_round estimado por CV: 800\n",
            "XGBoost final guardado: /content/models_saberpro/xgb_booster_final_fast.json\n",
            "Entrenando LightGBM sobre train+val...\n",
            "LightGBM guardado: /content/models_saberpro/lgb_model_fast.txt\n",
            "Generando probabilidades en test y promediando (ensemble)...\n",
            "Submission ensemble guardado en: /content/drive/MyDrive/DataProyectoIA/submission_ensemble_fast.csv\n",
            "       ID RENDIMIENTO_GLOBAL\n",
            "0  550236               bajo\n",
            "1   98545         medio-alto\n",
            "2  499179               alto\n",
            "3  782980               bajo\n",
            "4  785185               bajo\n",
            "Accuracy hold-out del ensemble (rápido): 0.4919180421172453\n",
            "Proceso rápido completado. Revisa submission_ensemble_fast.csv en tu Drive.\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Notebook Colab: Versión rápida y corregida (70/30 split)\n",
        "# Optuna rápido + XGBoost + LightGBM + Ensemble\n",
        "# Pegar y ejecutar en UNA sola celda en Colab.\n",
        "# =========================\n",
        "\n",
        "# ---------- 0) Instalación de dependencias ----------\n",
        "import sys, subprocess, os, time\n",
        "\n",
        "def pip_install(packages):\n",
        "    to_install = []\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            __import__(pkg.split('==')[0])\n",
        "        except Exception:\n",
        "            to_install.append(pkg)\n",
        "    if to_install:\n",
        "        print(\"Instalando:\", to_install)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *to_install])\n",
        "    else:\n",
        "        print(\"Dependencias OK\")\n",
        "\n",
        "pip_install([\"category_encoders\", \"xgboost\", \"lightgbm\", \"optuna\", \"joblib\", \"seaborn\"])\n",
        "\n",
        "# ---------- 1) Imports ----------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# TargetEncoder fallback\n",
        "try:\n",
        "    from category_encoders import TargetEncoder\n",
        "    has_target_encoder = True\n",
        "except Exception as e:\n",
        "    print(\"category_encoders no disponible -> OneHotEncoder fallback. Error:\", e)\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    has_target_encoder = False\n",
        "\n",
        "# xgboost / lightgbm / optuna\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "\n",
        "# ---------- 2) Montar Google Drive ----------\n",
        "from google.colab import drive\n",
        "print(\"Montando Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ---------- 3) Rutas ----------\n",
        "DRIVE_PATH = '/content/drive/MyDrive/DataProyectoIA'\n",
        "TRAIN_PATH = os.path.join(DRIVE_PATH, 'train_limpio.csv')\n",
        "TEST_PATH = os.path.join(DRIVE_PATH, 'test.csv')\n",
        "\n",
        "if not os.path.exists(DRIVE_PATH):\n",
        "    print(\"ERROR: No existe la carpeta esperada:\", DRIVE_PATH)\n",
        "    print(\"Contenido de /content/drive/MyDrive:\")\n",
        "    try:\n",
        "        print(os.listdir('/content/drive/MyDrive'))\n",
        "    except Exception:\n",
        "        pass\n",
        "    raise FileNotFoundError(\"Ajusta DRIVE_PATH\")\n",
        "\n",
        "if not os.path.exists(TRAIN_PATH):\n",
        "    print(\"ERROR: No se encontró:\", TRAIN_PATH)\n",
        "    print(\"Contenido de\", DRIVE_PATH, \":\", os.listdir(DRIVE_PATH))\n",
        "    raise FileNotFoundError(\"Coloca train_limpio.csv en la carpeta o corrige la ruta\")\n",
        "\n",
        "# ---------- 4) Cargar datos ----------\n",
        "print(\"Cargando:\", TRAIN_PATH)\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "print(\"Train shape:\", train.shape)\n",
        "\n",
        "test = None\n",
        "if os.path.exists(TEST_PATH):\n",
        "    print(\"Cargando:\", TEST_PATH)\n",
        "    test = pd.read_csv(TEST_PATH)\n",
        "    print(\"Test shape:\", test.shape)\n",
        "else:\n",
        "    print(\"No se encontró test.csv; no se generará submission automáticamente.\")\n",
        "\n",
        "# ---------- 5) Preparar X, y ----------\n",
        "if 'RENDIMIENTO_GLOBAL' not in train.columns:\n",
        "    raise KeyError(\"Falta columna 'RENDIMIENTO_GLOBAL' en train_limpio.csv\")\n",
        "\n",
        "X = train.drop(columns=['RENDIMIENTO_GLOBAL']).copy()\n",
        "y = train['RENDIMIENTO_GLOBAL'].copy()\n",
        "\n",
        "if 'ID' in X.columns:\n",
        "    train_ids = X['ID'].copy()\n",
        "    X = X.drop(columns=['ID'])\n",
        "else:\n",
        "    train_ids = None\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "print(\"Clases:\", list(le.classes_))\n",
        "\n",
        "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(\"Num cols:\", len(num_cols), \"Cat cols:\", len(cat_cols))\n",
        "\n",
        "# ---------- 6) Split (train/val estratificado 70/30) ----------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_enc, test_size=0.3, random_state=42, stratify=y_enc\n",
        ")\n",
        "print(\"Split realizado (70/30):\", X_train.shape, X_val.shape)\n",
        "\n",
        "# ---------- 7) Preprocesado ----------\n",
        "encoder = None\n",
        "ct = None\n",
        "\n",
        "if has_target_encoder and len(cat_cols) > 0:\n",
        "    encoder = TargetEncoder(cols=cat_cols, smoothing=0.3)\n",
        "    X_train_enc = encoder.fit_transform(X_train, y_train)\n",
        "    X_val_enc = encoder.transform(X_val)\n",
        "    if test is not None:\n",
        "        if 'ID' in test.columns:\n",
        "            test_ids = test['ID']\n",
        "            X_test_raw = test.drop(columns=['ID'])\n",
        "        else:\n",
        "            test_ids = None\n",
        "            X_test_raw = test.copy()\n",
        "        X_test_enc = encoder.transform(X_test_raw)\n",
        "else:\n",
        "    if len(cat_cols) > 0:\n",
        "        print(\"Usando OneHotEncoder (fallback)\")\n",
        "        from sklearn.preprocessing import OneHotEncoder\n",
        "        from sklearn.compose import ColumnTransformer\n",
        "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "        ct = ColumnTransformer(transformers=[('ohe', ohe, cat_cols)], remainder='passthrough')\n",
        "        X_train_tmp = ct.fit_transform(X_train)\n",
        "        X_val_tmp = ct.transform(X_val)\n",
        "        ohe_cols = ct.named_transformers_['ohe'].get_feature_names_out(cat_cols).tolist()\n",
        "        remainder_cols = [c for c in X_train.columns if c not in cat_cols]\n",
        "        cols_final = ohe_cols + remainder_cols\n",
        "        X_train_enc = pd.DataFrame(X_train_tmp, columns=cols_final)\n",
        "        X_val_enc = pd.DataFrame(X_val_tmp, columns=cols_final)\n",
        "        if test is not None:\n",
        "            if 'ID' in test.columns:\n",
        "                test_ids = test['ID']\n",
        "                X_test_raw = test.drop(columns=['ID'])\n",
        "            else:\n",
        "                test_ids = None\n",
        "                X_test_raw = test.copy()\n",
        "            X_test_tmp = ct.transform(X_test_raw)\n",
        "            X_test_enc = pd.DataFrame(X_test_tmp, columns=cols_final)\n",
        "    else:\n",
        "        X_train_enc = X_train.copy()\n",
        "        X_val_enc = X_val.copy()\n",
        "        if test is not None:\n",
        "            if 'ID' in test.columns:\n",
        "                test_ids = test['ID']\n",
        "                X_test_enc = test.drop(columns=['ID'])\n",
        "            else:\n",
        "                test_ids = None\n",
        "                X_test_enc = test.copy()\n",
        "\n",
        "# reset indices\n",
        "X_train_enc.reset_index(drop=True, inplace=True)\n",
        "X_val_enc.reset_index(drop=True, inplace=True)\n",
        "if test is not None:\n",
        "    X_test_enc.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# ---------- 8) Escalado ----------\n",
        "scaler = StandardScaler()\n",
        "if len(num_cols) > 0:\n",
        "    common_num_cols = [c for c in num_cols if c in X_train_enc.columns]\n",
        "    if len(common_num_cols) > 0:\n",
        "        X_train_enc[common_num_cols] = scaler.fit_transform(X_train_enc[common_num_cols])\n",
        "        X_val_enc[common_num_cols] = scaler.transform(X_val_enc[common_num_cols])\n",
        "        if test is not None:\n",
        "            X_test_enc[common_num_cols] = scaler.transform(X_test_enc[common_num_cols])\n",
        "\n",
        "# ---------- 9) Entrenamiento inicial rápido (baseline) ----------\n",
        "import numpy as _np\n",
        "X_train_arr = X_train_enc.values if hasattr(X_train_enc, \"values\") else _np.array(X_train_enc)\n",
        "X_val_arr = X_val_enc.values if hasattr(X_val_enc, \"values\") else _np.array(X_val_enc)\n",
        "if test is not None:\n",
        "    X_test_arr = X_test_enc.values if hasattr(X_test_enc, \"values\") else _np.array(X_test_enc)\n",
        "\n",
        "# Intentamos entrenar un XGBoost rápido como baseline\n",
        "baseline = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(le.classes_),\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "try:\n",
        "    baseline.fit(X_train_arr, y_train, eval_set=[(X_val_arr, y_val)], verbose=50)\n",
        "    baseline_ok = True\n",
        "except Exception as e:\n",
        "    print(\"Baseline XGBClassifier.fit falló (usando fallback xgb.train()). Error:\", e)\n",
        "    baseline_ok = False\n",
        "    dtrain_bl = xgb.DMatrix(X_train_arr, label=y_train)\n",
        "    dval_bl = xgb.DMatrix(X_val_arr, label=y_val)\n",
        "    booster_bl = xgb.train({\"objective\":\"multi:softprob\",\"num_class\":len(le.classes_),\"eta\":0.05,\"max_depth\":6,\"eval_metric\":\"mlogloss\"},\n",
        "                           dtrain_bl, num_boost_round=300, evals=[(dtrain_bl,\"train\"),(dval_bl,\"eval\")], verbose_eval=50)\n",
        "    class WrappedBoosterBl:\n",
        "        def __init__(self, booster_obj):\n",
        "            self.booster = booster_obj\n",
        "        def predict(self, X):\n",
        "            d = xgb.DMatrix(X)\n",
        "            probs = self.booster.predict(d)\n",
        "            return _np.argmax(probs, axis=1)\n",
        "    baseline = WrappedBoosterBl(booster_bl)\n",
        "\n",
        "# Evaluación baseline\n",
        "y_val_pred = baseline.predict(X_val_arr)\n",
        "acc_baseline = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Accuracy validación (baseline):\", acc_baseline)\n",
        "\n",
        "# ---------- 10) Optuna rápido (submuestra) con pruner y timeout ----------\n",
        "# Parámetros rápidos (ajustables)\n",
        "SAMPLE_SIZE = 150_000       # submuestra para Optuna (ajusta si memoria)\n",
        "N_TRIALS_FAST = 40          # número de trials rápidos\n",
        "NUM_BOOST_ROUND_FAST = 600  # rounds por trial\n",
        "EARLY_STOP_FAST = 30        # early stopping por trial\n",
        "OPTUNA_TIMEOUT = 3600       # segundos totales máximo (1 hora)\n",
        "\n",
        "print(\"Preparando Optuna rápido (submuestra). Esto tarda menos que explorar todo el dataset.\")\n",
        "\n",
        "# Crear submuestra reproducible de X_train_arr\n",
        "n_rows = len(X_train_arr)\n",
        "sample_n = int(min(SAMPLE_SIZE, n_rows))\n",
        "rng = np.random.RandomState(42)\n",
        "idx = rng.choice(n_rows, size=sample_n, replace=False)\n",
        "X_sub = X_train_arr[idx]\n",
        "y_sub = np.array(y_train)[idx]\n",
        "\n",
        "# split interno\n",
        "X_tr_s, X_va_s, y_tr_s, y_va_s = train_test_split(X_sub, y_sub, test_size=0.2, random_state=42, stratify=y_sub)\n",
        "\n",
        "# Objective con soporte de pruning\n",
        "def objective_fast(trial):\n",
        "    params = {\n",
        "        \"objective\": \"multi:softprob\",\n",
        "        \"num_class\": len(le.classes_),\n",
        "        \"eta\": trial.suggest_float(\"eta\", 1e-3, 0.2, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"eval_metric\": \"mlogloss\",\n",
        "        \"verbosity\": 0\n",
        "    }\n",
        "    dtr = xgb.DMatrix(X_tr_s, label=y_tr_s)\n",
        "    dva = xgb.DMatrix(X_va_s, label=y_va_s)\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtr,\n",
        "        num_boost_round=NUM_BOOST_ROUND_FAST,\n",
        "        evals=[(dtr,\"train\"), (dva,\"eval\")],\n",
        "        early_stopping_rounds=EARLY_STOP_FAST,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    preds = booster.predict(dva)\n",
        "    pred_idx = np.argmax(preds, axis=1)\n",
        "    return accuracy_score(y_va_s, pred_idx)\n",
        "\n",
        "# Crear study con MedianPruner y timeout\n",
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
        "study_fast = optuna.create_study(direction=\"maximize\", study_name=\"xgb_opt_fast\", pruner=pruner)\n",
        "\n",
        "print(\"Ejecutando Optuna (rápido). Timeout:\", OPTUNA_TIMEOUT, \"s. Trials máximos:\", N_TRIALS_FAST)\n",
        "t0 = time.time()\n",
        "study_fast.optimize(objective_fast, n_trials=N_TRIALS_FAST, n_jobs=1, timeout=OPTUNA_TIMEOUT)\n",
        "t1 = time.time()\n",
        "print(\"Optuna rápido finalizado. Tiempo (s):\", int(t1-t0))\n",
        "print(\"Trials completados:\", len(study_fast.trials))\n",
        "print(\"Mejor accuracy (submuestra):\", study_fast.best_value)\n",
        "print(\"Mejores parámetros (submuestra):\", study_fast.best_params)\n",
        "\n",
        "# Guardar estudio rápido\n",
        "OUT_DIR = \"/content/models_saberpro\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "joblib.dump(study_fast, os.path.join(OUT_DIR, \"optuna_xgb_fast.joblib\"))\n",
        "\n",
        "# ---------- 11) Reentrenar XGBoost final sobre train+val con mejores parámetros ----------\n",
        "print(\"Reentrenando XGBoost final sobre train+val con mejores parámetros...\")\n",
        "best = study_fast.best_params.copy()\n",
        "best[\"objective\"] = \"multi:softprob\"\n",
        "best[\"num_class\"] = len(le.classes_)\n",
        "best[\"eval_metric\"] = \"mlogloss\"\n",
        "best[\"verbosity\"] = 0\n",
        "\n",
        "X_full = np.vstack([X_train_arr, X_val_arr])\n",
        "y_full = np.concatenate([np.array(y_train), np.array(y_val)])\n",
        "dfull = xgb.DMatrix(X_full, label=y_full)\n",
        "\n",
        "# Estimar rounds óptimos con xgb.cv (rápido)\n",
        "cvres = xgb.cv(\n",
        "    params=best,\n",
        "    dtrain=dfull,\n",
        "    num_boost_round=800,\n",
        "    nfold=5,\n",
        "    stratified=True,\n",
        "    early_stopping_rounds=EARLY_STOP_FAST,\n",
        "    verbose_eval=False,\n",
        "    as_pandas=True,\n",
        "    seed=42\n",
        ")\n",
        "best_nrounds = len(cvres)\n",
        "print(\"num_boost_round estimado por CV:\", best_nrounds)\n",
        "\n",
        "# Entrenar booster final\n",
        "booster_final = xgb.train(params=best, dtrain=dfull, num_boost_round=best_nrounds, verbose_eval=50)\n",
        "booster_final.save_model(os.path.join(OUT_DIR, \"xgb_booster_final_fast.json\"))\n",
        "print(\"XGBoost final guardado:\", os.path.join(OUT_DIR, \"xgb_booster_final_fast.json\"))\n",
        "\n",
        "# ---------- 12) Entrenar LightGBM sobre train+val ----------\n",
        "print(\"Entrenando LightGBM sobre train+val...\")\n",
        "lgb_train = lgb.Dataset(X_full, label=y_full)\n",
        "lgb_params = {\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"num_class\": len(le.classes_),\n",
        "    \"metric\": \"multi_logloss\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"num_leaves\": 128,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"verbose\": -1,\n",
        "    \"min_data_in_leaf\": 20,\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"lambda_l2\": 0.1\n",
        "}\n",
        "lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=best_nrounds)\n",
        "lgb_model.save_model(os.path.join(OUT_DIR, \"lgb_model_fast.txt\"))\n",
        "print(\"LightGBM guardado:\", os.path.join(OUT_DIR, \"lgb_model_fast.txt\"))\n",
        "\n",
        "# ---------- 13) Ensemble sobre test (promedio de probabilidades) ----------\n",
        "print(\"Generando probabilidades en test y promediando (ensemble)...\")\n",
        "if not ('X_test_arr' in globals()):\n",
        "    X_test_arr = X_test_enc.values if hasattr(X_test_enc, \"values\") else np.array(X_test_enc)\n",
        "\n",
        "dtest = xgb.DMatrix(X_test_arr)\n",
        "probs_xgb_test = booster_final.predict(dtest)\n",
        "probs_lgb_test = lgb_model.predict(X_test_arr)\n",
        "\n",
        "if probs_xgb_test.shape != probs_lgb_test.shape:\n",
        "    print(\"Warning: shapes de probabilidades diferentes:\", probs_xgb_test.shape, probs_lgb_test.shape)\n",
        "\n",
        "avg_probs_test = (probs_xgb_test + probs_lgb_test) / 2.0\n",
        "preds_test_idx = np.argmax(avg_probs_test, axis=1)\n",
        "preds_test_labels = le.inverse_transform(preds_test_idx)\n",
        "\n",
        "submission_ens = pd.DataFrame({\n",
        "    \"ID\": test_ids.values if hasattr(test_ids, \"values\") else test_ids,\n",
        "    \"RENDIMIENTO_GLOBAL\": preds_test_labels\n",
        "})\n",
        "\n",
        "# Validaciones y guardado\n",
        "errors = []\n",
        "if len(submission_ens) != len(test):\n",
        "    errors.append(f\"Mismatch longitudes: submission={len(submission_ens)} test={len(test)}\")\n",
        "try:\n",
        "    ids_equal = submission_ens['ID'].astype(str).tolist() == test['ID'].astype(str).tolist()\n",
        "except Exception:\n",
        "    ids_equal = False\n",
        "if not ids_equal:\n",
        "    try:\n",
        "        submission_ens = submission_ens.set_index(\"ID\").loc[test['ID']].reset_index()\n",
        "    except Exception as e:\n",
        "        errors.append(\"No se pudo reordenar submission según test[ID]: \" + str(e))\n",
        "if submission_ens['ID'].duplicated().any():\n",
        "    errors.append(\"IDs duplicados en submission\")\n",
        "valid_classes = set(le.classes_)\n",
        "if not set(submission_ens['RENDIMIENTO_GLOBAL']).issubset(valid_classes):\n",
        "    errors.append(\"Etiquetas en submission no coinciden con clases del encoder\")\n",
        "\n",
        "if len(errors) == 0:\n",
        "    path_ens = os.path.join(DRIVE_PATH, \"submission_ensemble_fast.csv\")\n",
        "    submission_ens.to_csv(path_ens, index=False)\n",
        "    print(\"Submission ensemble guardado en:\", path_ens)\n",
        "    print(submission_ens.head())\n",
        "else:\n",
        "    print(\"Errores detectados, no se guardó submission:\")\n",
        "    for e in errors:\n",
        "        print(\"-\", e)\n",
        "\n",
        "# ---------- 14) Evaluar ensemble en hold-out ----------\n",
        "dval = xgb.DMatrix(X_val_arr)\n",
        "probs_xgb_val = booster_final.predict(dval)\n",
        "probs_lgb_val = lgb_model.predict(X_val_arr)\n",
        "avg_probs_val = (probs_xgb_val + probs_lgb_val) / 2.0\n",
        "preds_ens_val = np.argmax(avg_probs_val, axis=1)\n",
        "acc_ens = accuracy_score(y_val, preds_ens_val)\n",
        "print(\"Accuracy hold-out del ensemble (rápido):\", acc_ens)\n",
        "\n",
        "# ---------- 15) Guardar artefactos ----------\n",
        "joblib.dump(study_fast, os.path.join(OUT_DIR, \"optuna_xgb_fast.joblib\"))\n",
        "joblib.dump(le, os.path.join(OUT_DIR, \"label_encoder.joblib\"))\n",
        "joblib.dump(scaler, os.path.join(OUT_DIR, \"scaler.joblib\"))\n",
        "if has_target_encoder and encoder is not None:\n",
        "    try:\n",
        "        joblib.dump(encoder, os.path.join(OUT_DIR, \"target_encoder.joblib\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "if ct is not None:\n",
        "    try:\n",
        "        joblib.dump(ct, os.path.join(OUT_DIR, \"column_transformer_ohe.joblib\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"Proceso rápido completado. Revisa submission_ensemble_fast.csv en tu Drive.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}
